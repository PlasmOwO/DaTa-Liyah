{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import unicodedata\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "import gspread\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_database(database_name : str, host : str) :\n",
    "    \"\"\"Create connexion to Mongo Database\n",
    "\n",
    "    Args:\n",
    "        database_name (str): Name of the database\n",
    "        host (str) : host string of your database, null for localhost\n",
    "\n",
    "    Returns:\n",
    "        Client connexion\n",
    "    \"\"\"\n",
    "    client = MongoClient(host=host)\n",
    "    return client[database_name]\n",
    "\n",
    "def get_collection(connection, collection_name : str) :\n",
    "    \"\"\"Connect to a Mongo collection\n",
    "\n",
    "    Args:\n",
    "        connection : Mongo client connection\n",
    "        collection_name (str): Name of the collection\n",
    "\n",
    "    Returns:\n",
    "        _type_: The collection\n",
    "    \"\"\"\n",
    "    return connection[collection_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team name constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_tags_dico = {\n",
    "    \"SCL\" : [\"scl\",\"scald\"],\n",
    "    \"IWG\" : [\"iwg\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciate Selenium web browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_options = webdriver.FirefoxOptions()\n",
    "driver_options.add_argument(\"--headless\")\n",
    "driver = webdriver.Firefox(options=driver_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBlue_picks_bans(soup : BeautifulSoup ,name : str) -> tuple:\n",
    "    \"\"\"Scrap the blue picks and bans from the soup\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The soup scraped from an URL\n",
    "        name (str): The name of the team\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing :\n",
    "        - An ordenated list of picks\n",
    "        - An ordenated list of bans\n",
    "        - The name of the team\n",
    "    \"\"\"\n",
    "    picks = soup.find(attrs={\"class\": \"roomPickColumn blue\"}).get_text(\"|\")\n",
    "    picks = picks.split(\"|\")\n",
    "    bans = [x['alt'] for x in soup.find(attrs={\"class\": \"roomBanRow blue\"}).find_all('img')]\n",
    "    return picks,bans, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRed_picks_bans(soup : BeautifulSoup ,name : str) -> tuple :\n",
    "    \"\"\"Scrap the red picks and bans from the soup\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The soup scraped from an URL\n",
    "        name (str): The name of the team\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing :\n",
    "        - An ordenated list of picks\n",
    "        - An ordenated list of bans\n",
    "        - The name of the team\n",
    "    \"\"\"\n",
    "    picks = soup.find(attrs={\"class\": \"roomPickColumn red\"}).get_text(\"|\")\n",
    "    picks = picks.split(\"|\")\n",
    "    bans = [x['alt'] for x in soup.find(attrs={\"class\": \"roomBanRow red\"}).find_all('img')]\n",
    "    return picks, bans, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_team_in_name(name : str,team_reference_dict : dict) -> str : \n",
    "    \"\"\"Check if a name of a team side is recorded in the dictionnary\n",
    "\n",
    "    Args:\n",
    "        name (str): The scraped name of a team used in the URL.\n",
    "        team_reference_dict (dict): The dict containing all the teams names.\n",
    "\n",
    "    Returns:\n",
    "        str: The tag of the team if it's already in the dictionnary.\n",
    "            Otherwise, just the name\n",
    "    \"\"\"\n",
    "    for tag, keywords in team_reference_dict.items():\n",
    "        for  word in keywords :\n",
    "            if word in name.lower()  :\n",
    "                return tag\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(name : str) -> str:\n",
    "    \"\"\"Check in the team name scraped if there is a date and return it.\n",
    "\n",
    "    Args:\n",
    "        name (str): The scraped name of the ally team used in the URL.\n",
    "\n",
    "    Returns:\n",
    "        str: If there is a date, the date format : ddmmyyyy_nbgame\n",
    "            Otherwise, NaT\n",
    "    \"\"\"\n",
    "    match = re.search(r'\\d', name)\n",
    "    if match :\n",
    "        digit_index = match.start()\n",
    "        return name[digit_index:]\n",
    "    return \"NaT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_side_by_tag(soup : BeautifulSoup ,team_reference_dict : dict) -> tuple :\n",
    "    \"\"\"Scrap the name of both team, get the tag if possible, the date and the side of each team.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The soup.\n",
    "        team_reference_dict (dict): The dict containing all the teams names.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing :\n",
    "            - Tag or name of the blue team\n",
    "            - Tag or name of the red team\n",
    "            - Date of the draft\n",
    "    \"\"\"\n",
    "\n",
    "    blue_text = unicodedata.normalize(\"NFKD\",soup.find(attrs={\"class\" : \"roomReadyBackground roomReadyBackgroundblue\"}).previous_sibling.get_text())\n",
    "    red_text = unicodedata.normalize(\"NFKD\",soup.find(attrs={\"class\" : \"roomReadyBackground roomReadyBackgroundred\"}).previous_sibling.get_text())\n",
    "    blue_team = detect_team_in_name(blue_text, team_reference_dict)\n",
    "    red_team = detect_team_in_name(red_text,team_reference_dict)\n",
    "\n",
    "    if blue_team == \"SCL\" :\n",
    "        game_date = extract_date(blue_text)\n",
    "    elif red_team == \"SCL\" :\n",
    "        game_date = extract_date(red_text)\n",
    "    else :\n",
    "        game_date = \"NaT\"\n",
    "    return blue_team, red_team, game_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_draft(draft_url : str,team_reference_dict : dict , mongo_collection):\n",
    "    \"\"\"Scrap a URL and get all the draft data. Insert data into a mongodb collection.\n",
    "\n",
    "    Args:\n",
    "        draft_url (str): Url of the draft (from https://draftlol.dawe.gg/)\n",
    "        team_reference_dict (dict): The dict containing all the teams names.\n",
    "        mongo_collection : The collection of where you store the drafts data\n",
    "\n",
    "    Returns: The ID of the new object inserted in the database\n",
    "    \"\"\"\n",
    "    driver.get(draft_url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    \n",
    "    teams_names = get_side_by_tag(soup,team_reference_dict)\n",
    "\n",
    "    blue = getBlue_picks_bans(soup,teams_names[0])\n",
    "    red = getRed_picks_bans(soup,teams_names[1])\n",
    "\n",
    "    draft_json = {\n",
    "        \"link\" : draft_url,\n",
    "        \"date\" : teams_names[2],\n",
    "        \"blue\" : \n",
    "            {\n",
    "                \"picks\" : blue[0],\n",
    "                \"bans\"  : blue[1],\n",
    "                \"team\"  : blue[2]\n",
    "            },\n",
    "        \"red\" :\n",
    "            {\n",
    "                \"picks\" : red[0],\n",
    "                \"bans\"  : red[1],\n",
    "                \"team\"  : red[2]\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return mongo_collection.insert_one(draft_json).inserted_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_exist(draft_url : str, mongo_collection) -> bool :\n",
    "    \"\"\"Check if the URL already exist in one of the object of the collection.\n",
    "\n",
    "    Args:\n",
    "        draft_url (str): The URL of the draft.\n",
    "        mongo_collection : The mongodb collection\n",
    "\n",
    "    Returns:\n",
    "        bool : Return True if the document exist\n",
    "            False in the other case.\n",
    "    \"\"\"\n",
    "    document = mongo_collection.find_one({\"link\" : draft_url})\n",
    "    if document !=None :\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n",
    "Get URL drafts from gsheets and check if the document already exist in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "gc = gspread.service_account(filename=os.getenv(\"GOOGLE_CREDENTIALS_PATH\"))\n",
    "\n",
    "sh = gc.open_by_key(os.getenv(\"SPREADSHEET_KEY\"))\n",
    "\n",
    "list_draft_url = list(chain.from_iterable(sh.worksheet(\"Historique de Scrim\").get(\"K2:M\")))\n",
    "\n",
    "connect = connect_database('lol_match_database', host=os.getenv(\"ATLAS_CONNEXION_STRING\"))\n",
    "drafts_collection = get_collection(connect,\"drafts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip the list with URL and a list containing the result of DOCUMENT_EXIST function for that list\n",
    "for url, exists in zip(list_draft_url, [document_exist(draft_url=url,mongo_collection=drafts_collection) for url in list_draft_url]) :\n",
    "    if not exists :\n",
    "        print(scraping_draft(draft_url=url,team_reference_dict=team_tags_dico, mongo_collection=drafts_collection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
