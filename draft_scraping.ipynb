{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import unicodedata\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "import gspread\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team name constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_tags_dico = {\n",
    "    \"SCL\" : [\"scl\",\"scald\"],\n",
    "    \"IWG\" : [\"iwg\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciate Selenium web browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_options = webdriver.FirefoxOptions()\n",
    "driver_options.add_argument(\"--headless\")\n",
    "driver = webdriver.Firefox(options=driver_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBlue_picks_bans(soup,name):\n",
    "    picks = soup.find(attrs={\"class\": \"roomPickColumn blue\"}).get_text(\"|\")\n",
    "    picks = picks.split(\"|\")\n",
    "    bans = [x['alt'] for x in soup.find(attrs={\"class\": \"roomBanRow blue\"}).find_all('img')]\n",
    "    return picks,bans, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRed_picks_bans(soup,name):\n",
    "    picks = soup.find(attrs={\"class\": \"roomPickColumn red\"}).get_text(\"|\")\n",
    "    picks = picks.split(\"|\")\n",
    "    bans = [x['alt'] for x in soup.find(attrs={\"class\": \"roomBanRow red\"}).find_all('img')]\n",
    "    return picks, bans, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_team_in_name(name,team_reference_dict, side) :\n",
    "\n",
    "    for tag, keywords in team_reference_dict.items():\n",
    "        for  word in keywords :\n",
    "            if word in name.lower()  :\n",
    "                return tag\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(name):\n",
    "\n",
    "    match = re.search(r'\\d', name)\n",
    "    if match :\n",
    "        digit_index = match.start()\n",
    "        return name[digit_index:]\n",
    "    return \"NaT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_side_by_tag(soup,team_reference_dict):\n",
    "\n",
    "    blue_text = unicodedata.normalize(\"NFKD\",soup.find(attrs={\"class\" : \"roomReadyBackground roomReadyBackgroundblue\"}).previous_sibling.get_text())\n",
    "    red_text = unicodedata.normalize(\"NFKD\",soup.find(attrs={\"class\" : \"roomReadyBackground roomReadyBackgroundred\"}).previous_sibling.get_text())\n",
    "    \n",
    "    blue_team = detect_team_in_name(blue_text, team_reference_dict, \"Blue\")\n",
    "    red_team = detect_team_in_name(red_text,team_reference_dict, \"Red\")\n",
    "\n",
    "    if blue_team == \"SCL\" :\n",
    "        game_date = extract_date(blue_text)\n",
    "    elif red_team == \"SCL\" :\n",
    "        game_date = extract_date(red_text)\n",
    "    else :\n",
    "        game_date = \"NaT\"\n",
    "    return blue_team, red_team, game_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_draft(draft_url,team_reference_dict):\n",
    "\n",
    "    driver.get(draft_url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    \n",
    "    teams_names = get_side_by_tag(soup,team_reference_dict)\n",
    "\n",
    "    blue = getBlue_picks_bans(soup,teams_names[0])\n",
    "    red = getRed_picks_bans(soup,teams_names[1])\n",
    "\n",
    "    draft_json = {\n",
    "        \"link\" : draft_url,\n",
    "        \"date\" : teams_names[2],\n",
    "        \"blue\" : \n",
    "            {\n",
    "                \"picks\" : blue[0],\n",
    "                \"bans\"  : blue[1],\n",
    "                \"team\"  : blue[2]\n",
    "            },\n",
    "        \"red\" :\n",
    "            {\n",
    "                \"picks\" : red[0],\n",
    "                \"bans\"  : red[1],\n",
    "                \"team\"  : red[2]\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return draft_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_exist(draft_url):\n",
    "    client = MongoClient()\n",
    "    draft_collection = client['lol_match_database']['drafts']\n",
    "    document = draft_collection.find_one({\"link\" : draft_url})\n",
    "    if document !=None :\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n",
    "Get data from gsheets and check if the document already exist in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "gc = gspread.service_account(filename=os.getenv(\"GOOGLE_CREDENTIALS_PATH\"))\n",
    "\n",
    "sh = gc.open_by_key(os.getenv(\"SPREADSHEET_KEY\"))\n",
    "\n",
    "list_draft_url = list(chain.from_iterable(sh.worksheet(\"Historique de Scrim\").get(\"K2:M\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip the list with URL and a list containing the result of DOCUMENT_EXIST function for that list\n",
    "for url, exists in zip(list_draft_url, list(map(document_exist,list_draft_url))) :\n",
    "    if not exists :\n",
    "        print(scraping_draft(draft_url=url,team_reference_dict=team_tags_dico))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO push into the database"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
